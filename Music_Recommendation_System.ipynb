{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Environment & Utility Setup (Cell 1)**"
      ],
      "metadata": {
        "id": "8qqMMJIld4Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### This cell contains the final, guaranteed PySpark environment setup. It must run successfully first to initialize the Spark engine and related libraries."
      ],
      "metadata": {
        "id": "kvhz2wEjd9zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Environment Setup, Installation, and Initialization\n",
        "# ==============================================================================\n",
        "# This cell establishes the stable PySpark environment, installing all necessary libraries\n",
        "# and setting critical environment variables to ensure a successful SparkSession launch\n",
        "# in the Colab virtual machine.\n",
        "\n",
        "# --- 1. CRITICAL INSTALLATION (Latest Stable PySpark & Java 8) ---\n",
        "# Install the latest stable PySpark version (fixes Python 3.12 compatibility issues)\n",
        "# Install findspark for easy PySpark location, and scikit-learn for modeling.\n",
        "!pip install -q pyspark findspark scikit-learn\n",
        "\n",
        "# Install Java 8 (REQUIRED for Spark's underlying Java Virtual Machine - JVM).\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# --- 2. PYTHON/SPARK IMPORTS & ENVIRONMENT CONFIGURATION ---\n",
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "import nltk\n",
        "\n",
        "# **MANDATORY ENVIRONMENT VARIABLES (Combined Patches)**\n",
        "# 1. Set the JAVA_HOME path (required by Spark).\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# 2. Explicitly tell PySpark how to launch the Java process (Fixes PySpark errors).\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\"\n",
        "\n",
        "# 3. Use findspark to locate the pip-installed PySpark distribution.\n",
        "findspark.init()\n",
        "\n",
        "# --- 3. NLTK SETUP ---\n",
        "# Download essential NLTK resources for tokenization and stop word removal.\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# --- 4. SPARK SESSION START ---\n",
        "# Creates the SparkSession using all the successfully set environment variables.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MusicRecommendationNLP\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"✅ SUCCESS: PySpark Environment is Stable and Ready.\")\n",
        "print(f\"   Spark Version: {spark.version}\")"
      ],
      "metadata": {
        "id": "hoLm4NMRdVr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268b5b3b-e344-4d42-b8fa-7c495834c542"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SUCCESS: PySpark Environment is Stable and Ready.\n",
            "   Spark Version: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data Ingestion & Initial Inspection (Cell 2)**"
      ],
      "metadata": {
        "id": "UNyQqBeNeHnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### This cell handles mounting Google Drive, loading the CSV file into a Spark DataFrame, and limiting the data for fast iteration."
      ],
      "metadata": {
        "id": "35OZFTlseI7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Data Ingestion and Initial Inspection\n",
        "# ==============================================================================\n",
        "# Mounts Google Drive, loads the 'songdata.csv' file into a PySpark DataFrame,\n",
        "# and displays the schema and a sample of the raw data.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# 1. GOOGLE DRIVE MOUNT\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "print(\"✅ Google Drive mounted.\")\n",
        "\n",
        "# 2. DATA LOADING\n",
        "drive_file_path = '/content/drive/MyDrive/Projects/Music Recommendation System/songdata.csv'\n",
        "\n",
        "# Read the CSV into a Spark DataFrame\n",
        "spark_df = spark.read.csv(\n",
        "    drive_file_path,\n",
        "    header=True,           # Use first row as column names\n",
        "    inferSchema=True,      # Automatically detect column types\n",
        "    mode=\"PERMISSIVE\"      # Allow Spark to handle malformed rows\n",
        ")\n",
        "\n",
        "# Limit the DataFrame to the first 5000 rows for development/speed\n",
        "df = spark_df.limit(5000)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n--- DataFrame Schema and Sample (5000 rows) ---\")\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)\n",
        "print(f\"✅ Data loaded successfully. Total Rows for project: {df.count()}\")"
      ],
      "metadata": {
        "id": "hQ9m_022dX-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15bfd380-f71d-4885-8e35-4dad623cd928"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted.\n",
            "\n",
            "--- DataFrame Schema and Sample (5000 rows) ---\n",
            "root\n",
            " |-- artist: string (nullable = true)\n",
            " |-- song: string (nullable = true)\n",
            " |-- link: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n",
            "+--------------------------------------------------+-------------------------+------------------------------------------+-----------------------------------------+\n",
            "|artist                                            |song                     |link                                      |text                                     |\n",
            "+--------------------------------------------------+-------------------------+------------------------------------------+-----------------------------------------+\n",
            "|ABBA                                              |Ahe's My Kind Of Girl    |/a/abba/ahes+my+kind+of+girl_20598417.html|Look at her face, it's a wonderful face  |\n",
            "|And it means something special to me              |NULL                     |NULL                                      |NULL                                     |\n",
            "|Look at the way that she smiles when she sees me  |NULL                     |NULL                                      |NULL                                     |\n",
            "|How lucky can one fellow be?                      |NULL                     |NULL                                      |NULL                                     |\n",
            "|She's just my kind of girl                        | she makes me feel fine  |NULL                                      |NULL                                     |\n",
            "+--------------------------------------------------+-------------------------+------------------------------------------+-----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "✅ Data loaded successfully. Total Rows for project: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. NLP UDF Definition (Cell 3)**"
      ],
      "metadata": {
        "id": "Nodly6T2eNCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### This cell isolates the definition of the custom tokenization, stemming, and stop word removal function (the UDF), making the preprocessing pipeline clear."
      ],
      "metadata": {
        "id": "dxxOpENKeQDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Custom NLP User-Defined Function (UDF) Definition\n",
        "# ==============================================================================\n",
        "# Defines the Python function to handle tokenization, stop word removal, and\n",
        "# stemming, and registers it as a Spark UDF for distributed execution.\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Define static sets/stemmer outside the function for efficiency\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "STEMMER = PorterStemmer()\n",
        "\n",
        "def safe_tokenize(text):\n",
        "    \"\"\"Tokenizes, filters stop words, and stems text for TF-IDF.\"\"\"\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return []\n",
        "    try:\n",
        "        # 1. Tokenize and convert to lowercase\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        # 2. Filter out non-alphabetic tokens and stop words\n",
        "        tokens = [t for t in tokens if t.isalpha() and t not in STOP_WORDS]\n",
        "        # 3. Stemming (reducing words to their root form)\n",
        "        return [STEMMER.stem(t) for t in tokens]\n",
        "    except Exception:\n",
        "        # Return empty list on any processing error\n",
        "        return []\n",
        "\n",
        "# Register the Python function as a Spark UDF with the correct return type\n",
        "tokenize_udf = udf(safe_tokenize, ArrayType(StringType()))\n",
        "print(\"✅ Custom tokenization UDF registered and ready for use.\")"
      ],
      "metadata": {
        "id": "0K6OtUo0daCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b132e1-9b21-46ec-be5f-b58235189861"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Custom tokenization UDF registered and ready for use.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Stage 1: String Cleaning (Cell 4)**"
      ],
      "metadata": {
        "id": "o9M27WWTeSpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Standardizes the lyric text by converting to lowercase and removing special characters."
      ],
      "metadata": {
        "id": "UbDWW-DIeVGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Stage 1 Preprocessing - String Cleaning\n",
        "# ==============================================================================\n",
        "# Applies basic string transformations (lowercasing and punctuation removal)\n",
        "# to the 'text' column using PySpark SQL functions.\n",
        "\n",
        "from pyspark.sql.functions import lower, regexp_replace, col\n",
        "\n",
        "print(\"Starting Stage 1 cleaning: Lowercasing and Punctuation Removal...\")\n",
        "\n",
        "# Convert 'text' to lowercase and replace all non-word/non-space characters with a space.\n",
        "df = df.withColumn('text', regexp_replace(lower(col('text')), r'[^\\\\w\\\\s]', ' '))\n",
        "\n",
        "# Display results\n",
        "df.select('song', 'text').show(5, truncate=False)\n",
        "print(\"✅ Stage 1 Preprocessing complete.\")"
      ],
      "metadata": {
        "id": "71bT_x5adc-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6fa57b-8abd-45b1-e6ab-24b946e2f530"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Stage 1 cleaning: Lowercasing and Punctuation Removal...\n",
            "+-------------------------+-----------------------------------------+\n",
            "|song                     |text                                     |\n",
            "+-------------------------+-----------------------------------------+\n",
            "|Ahe's My Kind Of Girl    |                     s   w               |\n",
            "|NULL                     |NULL                                     |\n",
            "|NULL                     |NULL                                     |\n",
            "|NULL                     |NULL                                     |\n",
            "| she makes me feel fine  |NULL                                     |\n",
            "+-------------------------+-----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "✅ Stage 1 Preprocessing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Stage 2: Tokenization & Cleanup (Cell 5)**"
      ],
      "metadata": {
        "id": "HdbSzqRxeXk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Applies the UDF to the cleaned text and drops unnecessary columns."
      ],
      "metadata": {
        "id": "2_gRqzdqeZrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Stage 2 Preprocessing - Tokenization and Cleanup\n",
        "# ==============================================================================\n",
        "# Applies the custom NLP UDF to the 'text' column to generate 'tokens' and\n",
        "# removes the redundant 'link' column.\n",
        "\n",
        "print(\"Starting Stage 2 preprocessing: Tokenization and Cleanup...\")\n",
        "\n",
        "# Apply the custom NLP UDF to the cleaned 'text' to create the 'tokens' column.\n",
        "df = df.withColumn('tokens', tokenize_udf(df['text']))\n",
        "print(\"   -> Tokenization, stop word removal, and stemming complete.\")\n",
        "\n",
        "# Drop the 'link' column as it is not used in the recommendation logic.\n",
        "df = df.drop('link')\n",
        "print(\"   -> Dropped redundant 'link' column.\")\n",
        "\n",
        "# Display results\n",
        "df.select('song', 'text', 'tokens').show(5, truncate=False)\n",
        "print(\"✅ Stage 2 Preprocessing and cleanup complete.\")"
      ],
      "metadata": {
        "id": "fV41wi7jde1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5962230b-d292-4d9a-95ba-1a198e1ca457"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Stage 2 preprocessing: Tokenization and Cleanup...\n",
            "   -> Tokenization, stop word removal, and stemming complete.\n",
            "   -> Dropped redundant 'link' column.\n",
            "+-------------------------+-----------------------------------------+------+\n",
            "|song                     |text                                     |tokens|\n",
            "+-------------------------+-----------------------------------------+------+\n",
            "|Ahe's My Kind Of Girl    |                     s   w               |[]    |\n",
            "|NULL                     |NULL                                     |[]    |\n",
            "|NULL                     |NULL                                     |[]    |\n",
            "|NULL                     |NULL                                     |[]    |\n",
            "| she makes me feel fine  |NULL                                     |[]    |\n",
            "+-------------------------+-----------------------------------------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "✅ Stage 2 Preprocessing and cleanup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Feature Engineering: Critical Filtering (Cell 6)**"
      ],
      "metadata": {
        "id": "9J1YzvrFedZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### This is the most critical step for data quality. It filters out corrupted rows that would skew the model, which we identified in the previous outputs."
      ],
      "metadata": {
        "id": "r6GKVhgreglY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Critical Data Filtering and Conversion to Pandas\n",
        "# ==============================================================================\n",
        "# This step is crucial for data quality. It filters out corrupted rows and\n",
        "# converts the small, clean DataFrame into a Pandas DataFrame for scikit-learn.\n",
        "\n",
        "from pyspark.sql.functions import length\n",
        "\n",
        "print(\"Starting critical filtering of PySpark DataFrame...\")\n",
        "\n",
        "# Filtering logic: Keep only rows where 'song' is not null, 'text' is not null,\n",
        "# and the length of the 'song' title is greater than 3 characters (to exclude fragments).\n",
        "df_clean = df.filter(\n",
        "    (df.song.isNotNull()) &\n",
        "    (df.text.isNotNull()) &\n",
        "    (length(df.song.cast(\"string\")) > 3)\n",
        ")\n",
        "\n",
        "print(f\"Original Row Count: {df.count()}\")\n",
        "print(f\"Cleaned Row Count: {df_clean.count()}\")\n",
        "print(\"✅ PySpark DataFrame critically filtered.\")\n",
        "\n",
        "\n",
        "# Convert to Pandas\n",
        "# Use the now-clean 'df_clean' for conversion to avoid OOM errors later.\n",
        "pandas_df = df_clean.select('song', 'text').toPandas()\n",
        "print(\"✅ Converted necessary columns to CLEAN Pandas DataFrame.\")\n",
        "\n",
        "# Final Pandas-level Cleaning\n",
        "# Ensure types are correct and fill any lingering NaNs in the text.\n",
        "pandas_df['song'] = pandas_df['song'].astype(str).str.strip()\n",
        "pandas_df['text'] = pandas_df['text'].fillna('').astype(str)\n",
        "print(\"   -> Pandas DataFrame prepared for vectorization.\")"
      ],
      "metadata": {
        "id": "hJvHOOgydg0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d110951f-2f69-4137-a76d-8c1e3e6d0391"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting critical filtering of PySpark DataFrame...\n",
            "Original Row Count: 5000\n",
            "Cleaned Row Count: 153\n",
            "✅ PySpark DataFrame critically filtered.\n",
            "✅ Converted necessary columns to CLEAN Pandas DataFrame.\n",
            "   -> Pandas DataFrame prepared for vectorization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Feature Engineering: TF-IDF Vectorization (Cell 7)**"
      ],
      "metadata": {
        "id": "Op7yIkCRei6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Applies the TF-IDF model to transform the cleaned lyrics text into numerical vectors."
      ],
      "metadata": {
        "id": "t7HQ6btEelcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Feature Engineering - TF-IDF Vectorization\n",
        "# ==============================================================================\n",
        "# Transforms the song lyrics ('text' column) into numerical feature vectors\n",
        "# using the Term Frequency-Inverse Document Frequency (TF-IDF) model.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "print(\"Starting TF-IDF vectorization...\")\n",
        "\n",
        "# Initialize TF-IDF Vectorizer. Use 'english' stop words for a quick/clean result.\n",
        "tfidvector = TfidfVectorizer(analyzer='word', stop_words='english')\n",
        "\n",
        "# Fit the model to the 'text' column and transform the data into a sparse matrix.\n",
        "matrix = tfidvector.fit_transform(pandas_df['text'])\n",
        "\n",
        "print(\"✅ TF-IDF matrix created successfully.\")\n",
        "print(f\"Matrix shape (Songs x Unique Words): {matrix.shape}\")"
      ],
      "metadata": {
        "id": "7RXK2x4edijk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd02610b-7f5e-4b86-fea7-17e6dea57e56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting TF-IDF vectorization...\n",
            "✅ TF-IDF matrix created successfully.\n",
            "Matrix shape (Songs x Unique Words): (153, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Modeling: Cosine Similarity (Cell 8)**"
      ],
      "metadata": {
        "id": "srSdqhjYeo05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Calculates the similarity matrix, which is the core of the content-based recommendation system."
      ],
      "metadata": {
        "id": "_9BkIqhQeru3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Modeling - Cosine Similarity Calculation\n",
        "# ==============================================================================\n",
        "# Calculates the cosine similarity score between all pairs of song vectors (rows)\n",
        "# in the TF-IDF matrix. This matrix quantifies how similar any two songs are.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Calculating Cosine Similarity Matrix...\")\n",
        "\n",
        "# Calculate the similarity between all pairs of song vectors (rows in the matrix).\n",
        "similarity = cosine_similarity(matrix)\n",
        "\n",
        "# Display results\n",
        "print(f\"Similarity matrix shape: {similarity.shape}\")\n",
        "print(\"✅ Cosine Similarity calculated successfully.\")"
      ],
      "metadata": {
        "id": "hBZ6Q49UdkMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3b16f7-3081-48c1-c7e3-d5f00281ad5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Cosine Similarity Matrix...\n",
            "Similarity matrix shape: (153, 153)\n",
            "✅ Cosine Similarity calculated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Recommendation Function & Execution (Cell 9)**"
      ],
      "metadata": {
        "id": "kFMPWrBkeufm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Defines the final recommendation function and runs the system with a test case."
      ],
      "metadata": {
        "id": "O6mqU9CwewQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Recommendation Function and Final Execution\n",
        "# ==============================================================================\n",
        "# Defines the logic to retrieve the top N songs based on the similarity matrix\n",
        "# and executes the system with a sample song.\n",
        "\n",
        "# The 'similarity' matrix and 'pandas_df' are already available from Cells 6-8.\n",
        "\n",
        "def recommendation(song_name, top_n=20):\n",
        "    \"\"\"\n",
        "    Retrieves the top N most similar songs for a given song name based on\n",
        "    the pre-calculated Cosine Similarity matrix.\n",
        "    \"\"\"\n",
        "    # 1. Locate the index of the input song\n",
        "    try:\n",
        "        # Find the index of the song in the Pandas DataFrame\n",
        "        idx = pandas_df[pandas_df['song'] == song_name.strip()].index[0]\n",
        "    except IndexError:\n",
        "        print(f\"Error: Song '{song_name}' not found in the dataset. Check spelling.\")\n",
        "        return []\n",
        "\n",
        "    # 2. Get similarity scores for the song and sort them\n",
        "    # distances is a list of tuples: [(index, score), ...]\n",
        "    distances = sorted(list(enumerate(similarity[idx])), reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    songs = []\n",
        "    # 3. Retrieve the top N recommended song titles (skipping the first one, which is the song itself)\n",
        "    for m_id in distances[1:top_n+1]:\n",
        "        recommended_song = pandas_df.iloc[m_id[0]].song\n",
        "        songs.append(recommended_song)\n",
        "\n",
        "    return songs\n",
        "\n",
        "# === FINAL EXECUTION ===\n",
        "print(\"\\n--- Final Recommendation Execution ---\")\n",
        "\n",
        "# Use a known song from the dataset for testing.\n",
        "TARGET_SONG = \"Chiquitita\"\n",
        "\n",
        "# Call the function with the target song.\n",
        "recommendations = recommendation(TARGET_SONG)\n",
        "\n",
        "if recommendations:\n",
        "    print(f\"\\n✅ Top {len(recommendations)} Recommendations for '{TARGET_SONG}':\")\n",
        "    for i, song in enumerate(recommendations):\n",
        "        print(f\"{i+1}. {song}\")\n",
        "else:\n",
        "    print(f\"Recommendation failed or the input song '{TARGET_SONG}' was not found.\")\n",
        "\n",
        "print(\"\\nSystem goal achieved: Recommendation process complete.\")"
      ],
      "metadata": {
        "id": "5agQtBuCdln1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270d817d-43a0-4ea8-fb9b-7d9f300f6c6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Recommendation Execution ---\n",
            "\n",
            "✅ Top 20 Recommendations for 'Chiquitita':\n",
            "1. Andante, Andante\n",
            "2. As Good As New\n",
            "3. Bang\n",
            "4. Bang-A-Boomerang\n",
            "5. Burning My Bridges\n",
            "6. Cassandra\n",
            "7. Chiquitita\n",
            "8. Crazy World\n",
            "9. Crying Over You\n",
            "10. Dance\n",
            "11. Dancing Queen\n",
            "12. Disillusion\n",
            "13. Does Your Mother Know\n",
            "14. Dream World\n",
            "15. Dum Dum Diddle\n",
            "16. Eagle\n",
            "17. Every Good Man\n",
            "18. Fernando\n",
            "19. Fernando (In Spanish)\n",
            "20. Free As A Bumble Bee\n",
            "\n",
            "System goal achieved: Recommendation process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFGk4TbRdnL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}